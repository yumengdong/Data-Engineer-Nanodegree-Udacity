# Data-Engineer-Nanodegree-Udacity
Udacity Data Analyst Nanodegree Projects

<img src="https://media-exp1.licdn.com/dms/image/C5612AQG2UUbFIFjo5Q/article-inline_image-shrink_1500_2232/0?e=1591833600&v=beta&t=YHNZbwDRffFK2aju70KOiAHJaGhXJHFIEelo8i_sIAA">

## Project 1. Data Modeling with Postgres
- <A href='https://github.com/yumengdong/Data-Engineer-Nanodegree-Udacity/blob/master/Project_data%20modeling%20with%20postgres'>Data Modeling with Postgres</A><BR>
  - Model user activity data to create a database in Postgres for a music streaming app
  - Developed a Star Schema database using optimized definitions of Fact and Dimension tables. Normalization of tables.
  - Define the Fact and Dimension tables, import song data and logs data stored in JSON files and insert data into tables
  - Write an ETL pipeline that transfers data into tables in Postgres using Python and SQL
  
## Project 2. Data Modeling with Cassandra
- <A href='https://github.com/yumengdong/Data-Engineer-Nanodegree-Udacity/tree/master/Project_Data%20Modeling%20with%20Apache%20Cassandra'>Data Modeling with Cassandra</A><BR>
  - Data modeling with Apache Cassandra and complete an ETL pipeline using Python
  - Developed denormalized tables optimized for a specific set queries and business needs
  - Model the data in JSON format and wrangle them into star schema in Apache Cassandra to run queries

## Project 3. Data Warehouses on AWS
- <A href='https://github.com/yumengdong/Data-Engineer-Nanodegree-Udacity/blob/master/Project_Data%20Warehouse'>Data Warehouses on AWS</A><BR>
  - Create an IMA role using python AWS SDK boto3
  - Create the AWS Redshift cluster and access the cluster endpoint using JupyterNotebook
  - Build an ETL that loads song json data from Amazon S3 to be processed into a star schema
  - Stages data in Amazon Redshift, then transforms data into dimensional tables

## Project 4. Data Lakes with Apache Spark
- <A href='https://github.com/yumengdong/Data-Engineer-Nanodegree-Udacity/blob/master/Project_Data%20Warehouse'>Data Lakes with Apache Spark</A><BR>
- Bild an ETL pipeline for a data lake. 
- Load data from S3, process the data into analytics tables using Spark, and load them back into S3.Â 
- Deploy this Spark process on a cluster using AWS.
